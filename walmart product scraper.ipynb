{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0791fe7b-2de1-4309-9067-3174e4c8c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import queue\n",
    "import time\n",
    "import random\n",
    "from requests.exceptions import HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12d7b8e9-a1cc-49c6-9d1c-f755f78f59e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.walmart.com\"\n",
    "OUTPUT_FILE = \"product_info.jsonl\"\n",
    "FAILED_LOG = \"failed_urls.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17cf47bb-476d-4d3f-8e1f-cc34135578b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:136.0) Gecko/20100101 Firefox/136.0/byA6CjeQHYiZ0Se\",\n",
    "    \"Mozilla/5.0 (Macintosh; PPC Mac OS X 10.4.4; rv:60.5.0) Gecko/20100101 Firefox/60.5.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:135.0) Gecko/20100101 Firefox/135.0/KYtdkVMAZUtmong-89\",\n",
    "    \"Mozilla/5.0 (X11; Linux; en-AU; rv:135.0) Gecko/20161700 Firefox/135.0\",\n",
    "    \"Mozilla/5.0 (iPad; CPU iPad OS 16_7_7 like Mac OS X) AppleWebKit/532.1 (KHTML, like Gecko) FxiOS/11.1o2152.0 Mobile/76S605 Safari/532.1\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; WOW64; x64; rv:121.0) Gecko/20100101 Firefox/121.0/CetKrEFQ34G7OAO-68\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72a95c9e-d659-4aa8-a49b-c12c20b2e05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_queries = [\n",
    "    \"computers\", \"laptops\", \"desktops\", \"monitors\", \"printers\", \"hard+drives\", \"usb\", \"cords\", \"cameras\",\n",
    "    \"mouse\", \"keyboard\", \"microphones\", \"speakers\", \"radio\", \"tablets\", \"android\", \"apple\", \"watch\", \"smart+watch\",\n",
    "    \"fridge\", \"airconditioning\", \"wifi\", \"router\", \"modem\", \"desk\", \"xbox\", \"playstation\", \"nintendo\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc648943-d15b-43d4-903c-7c654cdd79d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_queue = queue.Queue()\n",
    "seen_urls = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "489ab64a-04de-4c0f-bfdc-d46f1beb7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headers():\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"accept-language\": \"en-US\",\n",
    "        \"accept-encoding\": \"gzip, deflate, br, zstd\",\n",
    "        \"user-agent\": random.choice(USER_AGENTS)\n",
    "    }\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d73a0fa2-6773-487a-b831-e6f1a08a6d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_links_from_search_page(query, page_number):\n",
    "    search_url = f\"https://www.walmart.com/search?q={query}&page={page_number}\"\n",
    "    max_retries = 5\n",
    "    backoff_factor = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            headers = get_headers()\n",
    "            response = requests.get(search_url, headers=headers)\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            product_links = []\n",
    "\n",
    "            found = False\n",
    "            for a_tag in soup.find_all('a', href=True):\n",
    "                if '/ip/' in a_tag['href']:\n",
    "                    found = True\n",
    "                    full_url = a_tag['href'] if \"https\" in a_tag['href'] else BASE_URL + a_tag['href']\n",
    "                    if full_url not in seen_urls:\n",
    "                        product_links.append(full_url)\n",
    "\n",
    "            if not found:\n",
    "                print(\"\\n\\n\\nSOUP WHEN NOT FOUND\", soup)\n",
    "\n",
    "            return product_links\n",
    "\n",
    "        except HTTPError as e:\n",
    "            if e.response.status_code == 412:\n",
    "                print(f\"Precondition Failed (412): {e}. Skipping URL.\")\n",
    "                break\n",
    "            wait_time = backoff_factor ** attempt\n",
    "            print(f\"HTTP error: {e}. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get product links for query: {query} on page: {page_number}. Error: {e}\")\n",
    "            with open(FAILED_LOG, \"a\") as log:\n",
    "                log.write(f\"SEARCH_FAILED: {search_url}\\n\")\n",
    "            break\n",
    "\n",
    "    print(f\"Skipping query after {max_retries} retries: {query} on page: {page_number}\")\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0453b378-d82b-4708-aadc-52655dabd5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_info(product_url):\n",
    "    print(\"Processing URL\", product_url)\n",
    "    max_retries = 5\n",
    "    backoff_factor = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            headers = get_headers()\n",
    "            response = requests.get(product_url, headers=headers)\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            script_tag = soup.find('script', id='__NEXT_DATA__')\n",
    "\n",
    "            if script_tag is None:\n",
    "                return None\n",
    "\n",
    "            data = json.loads(script_tag.string)\n",
    "            initial_data = data[\"props\"][\"pageProps\"][\"initialData\"][\"data\"]\n",
    "            product_data = initial_data[\"product\"]\n",
    "            reviews_data = initial_data.get(\"reviews\", {})\n",
    "\n",
    "            product_info = {\n",
    "                \"price\": product_data[\"priceInfo\"][\"currentPrice\"][\"price\"],\n",
    "                \"review_count\": reviews_data.get(\"totalReviewCount\", 0),\n",
    "                \"item_id\": product_data[\"usItemId\"],\n",
    "                \"avg_rating\": reviews_data.get(\"averageOverallRating\", 0),\n",
    "                \"product_name\": product_data[\"name\"],\n",
    "                \"brand\": product_data.get(\"brand\", \"\"),\n",
    "                \"availability\": product_data[\"availabilityStatus\"],\n",
    "                \"image_url\": product_data[\"imageInfo\"][\"thumbnailUrl\"],\n",
    "                \"short_description\": product_data.get(\"shortDescription\", \"\")\n",
    "            }\n",
    "\n",
    "            return product_info\n",
    "\n",
    "        except HTTPError as e:\n",
    "            if e.response.status_code == 412:\n",
    "                print(f\"Precondition Failed (412): {e}. Skipping URL.\")\n",
    "                break\n",
    "            wait_time = backoff_factor ** attempt\n",
    "            print(f\"HTTP error: {e}. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process URL: {product_url}. Error: {e}\")\n",
    "            with open(FAILED_LOG, \"a\") as log:\n",
    "                log.write(f\"PRODUCT_FAILED: {product_url}\\n\")\n",
    "            break\n",
    "\n",
    "    print(f\"Skipping URL after {max_retries} retries: {product_url}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb29dba-3f5c-4879-9e86-7c439a4c9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    with open(OUTPUT_FILE, 'w') as file:\n",
    "        while search_queries:\n",
    "            current_query = search_queries.pop(0)\n",
    "            print(\"\\n\\nCURRENT QUERY\", current_query, \"\\n\\n\")\n",
    "            page_number = 1\n",
    "\n",
    "            while True:\n",
    "                product_links = get_product_links_from_search_page(current_query, page_number)\n",
    "                if not product_links or page_number > 99:\n",
    "                    print(f\"No products found on page {page_number}. Ending search for this query.\")\n",
    "                    break\n",
    "\n",
    "                for link in product_links:\n",
    "                    if link not in seen_urls:\n",
    "                        product_queue.put(link)\n",
    "                        seen_urls.add(link)\n",
    "\n",
    "                while not product_queue.empty():\n",
    "                    product_url = product_queue.get()\n",
    "                    product_info = extract_product_info(product_url)\n",
    "                    if product_info:\n",
    "                        file.write(json.dumps(product_info) + \"\\n\")\n",
    "\n",
    "                page_number += 1\n",
    "                print(\"Next Page:\", page_number)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
